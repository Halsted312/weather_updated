# Handoff: Inference Pipeline Alignment Complete

**Date**: 2025-12-09
**Previous Agent**: kalshi-weather-quant
**Status**: Inference pipeline aligned, data gaps identified

---

## What Was Done

### Inference Pipeline Now Aligned with Training

The inference pipeline was using a different code path (`snapshot_builder.py`) than training (`pipeline.py`), resulting in 127/220 features missing. This has been fixed.

**Key changes made:**

| File | Change |
|------|--------|
| `models/data/loader.py` | Added `load_full_inference_data()` - loads all 8 data sources |
| `models/data/loader.py` | Added `load_historical_lag_data()` - returns DataFrame for `compute_lag_features()` |
| `models/data/loader.py` | Added `load_candles_for_inference()` - public function using `CityConfig` |
| `models/data/loader.py` | Added `load_city_observations_for_inference()` - public function |
| `models/data/loader.py` | Added deprecation warning to old `load_inference_data()` |
| `models/data/loader.py` | Added `_to_naive()` helper for datetime normalization |
| `models/data/dataset.py` | `_load_candles()` now delegates to `load_candles_for_inference()` |
| `models/inference/live_engine.py` | Fixed `_get_snapshot_params()` to use 5-min intervals |
| `models/inference/live_engine.py` | Removed dead `_compute_std()` method |
| `test_inference_parity.py` | Created test script for feature parity validation |

### Test Results

```
python test_inference_parity.py chicago 2025-11-15

Expected: 220 features
Actual: 254 features
Missing: 0            # SUCCESS - all 220 model features present
Extra: 34             # OK - extra features ignored by model
```

**Feature parity achieved: 220/220 features computed**

---

## Known Data Gaps (NOT Code Issues)

### 1. Cloudcover is NULL in minute observations

```sql
-- All NULL in wx.vc_minute_weather for data_type='actual_obs'
SELECT cloudcover, COUNT(*) FROM wx.vc_minute_weather
WHERE data_type = 'actual_obs' GROUP BY cloudcover;
-- Result: NULL: 6912 rows
```

**Why**: Visual Crossing only provides cloudcover in **hourly** data, not minute data. Since we use 5-minute obs snapshots, cloudcover is naturally NULL.

**Impact**: 19 cloudcover-derived features are NULL:
- `cloudcover_mean_last_60min`
- `cloudcover_rate_last_30min`
- `cloudcover_volatility_60min`
- `cloud_stability_score`
- `clouding_trend_flag`
- `clear_sky_flag`
- etc.

**Decision**: Leave as NULL. CatBoost handles NaN natively. These features will have reduced signal but model still works.

### 2. NDFD Guidance Not Backfilled

```python
more_apis = {
    'nbm': {'latest_run': Yes},   # Working
    'hrrr': {'latest_run': Yes},  # Working
    'ndfd': {'latest_run': No},   # NOT AVAILABLE
}
```

**Impact**: `ndfd_*` features are NULL:
- `ndfd_tmax_T1_f`
- `ndfd_drift_T2_to_T1_f`

**Decision**: For later. NBM and HRRR are more important anyway.

### 3. Settlement Data Lag

Settlements are only available up to ~5 days ago (IEM source):
```
2025-12-03: 30°F (iem)  # Most recent
2025-12-02: 25°F (iem)
...
```

**Impact**: For live inference on today's date, lag features (`settle_f_lag1`, etc.) may be NULL if settlement ingestion hasn't run.

---

## What Next Agent Needs To Do

### Priority 1: Ensure Data is Current and Backfilled

The inference code is correct. The nulls are from missing data in the DB.

#### Check Data Freshness

```bash
# Check most recent observations
python -c "
from src.db.connection import get_db_session
from sqlalchemy import text

with get_db_session() as session:
    result = session.execute(text('''
        SELECT city_code, MAX(datetime_local) as latest
        FROM wx.vc_minute_weather vmw
        JOIN wx.vc_location vl ON vmw.vc_location_id = vl.id
        WHERE data_type = 'actual_obs'
        GROUP BY city_code
        ORDER BY city_code
    ''')).fetchall()
    print('=== Latest Observations ===')
    for row in result:
        print(f'{row[0]}: {row[1]}')
"
```

```bash
# Check most recent settlements
python -c "
from src.db.connection import get_db_session
from sqlalchemy import text

with get_db_session() as session:
    result = session.execute(text('''
        SELECT city, MAX(date_local) as latest
        FROM wx.settlement
        GROUP BY city
        ORDER BY city
    ''')).fetchall()
    print('=== Latest Settlements ===')
    for row in result:
        print(f'{row[0]}: {row[1]}')
"
```

#### Backfill Commands (if needed)

```bash
# Backfill VC observations (if gaps exist)
python scripts/ingest_vc_obs_backfill.py --city chicago --start 2025-12-01 --end 2025-12-09

# Backfill settlements
python scripts/ingest_settlement_multi.py --city chicago

# Backfill historical forecasts (for multi-horizon features)
python scripts/ingest_vc_historical_forecast_parallel.py --city chicago
```

#### Start Live Ingestion

Ensure these are running continuously:

```bash
# Live VC observations (should run every 5 min via cron/systemd)
python scripts/ingest_vc_obs_live.py

# Live settlements (should run daily after market close)
python scripts/ingest_settlement_multi.py --live
```

Check systemd services:
```bash
systemctl status weather-obs-ingest.service
systemctl status weather-settlement-ingest.service
```

### Priority 2: Verify Inference Works End-to-End

After data is current, test the full inference pipeline:

```bash
# Test feature parity (should pass with 0 missing)
python test_inference_parity.py chicago

# Test for all cities
for city in chicago austin denver los_angeles miami philadelphia; do
    echo "=== $city ==="
    python test_inference_parity.py $city 2025-12-03
done
```

### ~~Priority 3: Update live_engine.py to Add Lag Features~~ ✅ DONE

The `live_engine.py` now computes lag features via `compute_lag_features()` after `compute_snapshot_features()`. This was added on 2025-12-09.

---

## Files Modified (for reference)

```
models/data/loader.py
  - load_full_inference_data()      # New - main inference loader
  - load_historical_lag_data()      # New - for lag features
  - load_candles_for_inference()    # New - public candle loader
  - load_city_observations_for_inference()  # New - public city obs loader
  - load_inference_data()           # Added deprecation warning
  - _to_naive()                     # New - datetime helper

models/data/dataset.py
  - _load_candles()                 # Now delegates to loader.py

models/inference/live_engine.py
  - _get_snapshot_params()          # Fixed to use 5-min intervals
  - Removed dead _compute_std()

test_inference_parity.py            # New test script
```

---

## Quick Verification Commands

```bash
# 1. Check inference produces all 220 features
python test_inference_parity.py chicago 2025-12-03

# 2. Check data freshness
python scripts/check_data_freshness.py

# 3. Run a simple inference test
python -c "
from datetime import date, datetime, time
from models.data.loader import load_full_inference_data
from src.db.connection import get_db_session

with get_db_session() as session:
    data = load_full_inference_data('chicago', date(2025, 12, 3),
                                     datetime(2025, 12, 3, 14, 30), session)
    print(f'Obs: {len(data[\"temps_sofar\"])}')
    print(f'Forecast: {\"Yes\" if data[\"fcst_daily\"] else \"No\"}')
    print(f'Candles: {\"Yes\" if data[\"candles_df\"] is not None else \"No\"}')
    print(f'Lag data: {len(data[\"lag_data\"])} days')
"
```

---

## Summary

| Item | Status |
|------|--------|
| Inference uses same code path as training | ✅ DONE |
| 220/220 features computed | ✅ DONE |
| Lag features via existing `compute_lag_features()` | ✅ DONE |
| Cloudcover nulls | EXPECTED (hourly only) |
| NDFD nulls | DEFERRED |
| Data backfilled to current | **NEEDS VERIFICATION** |
| Live ingestion running | **NEEDS VERIFICATION** |
| `live_engine.py` adds lag features | ✅ DONE |

**All code changes complete. Next agent should focus on data freshness and live ingestion only.**
